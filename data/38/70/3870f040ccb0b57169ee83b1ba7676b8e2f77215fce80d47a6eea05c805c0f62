{"url":"https://api.github.com/repos/bio-guoda/preston/issues/139","repository_url":"https://api.github.com/repos/bio-guoda/preston","labels_url":"https://api.github.com/repos/bio-guoda/preston/issues/139/labels{/name}","comments_url":"https://api.github.com/repos/bio-guoda/preston/issues/139/comments","events_url":"https://api.github.com/repos/bio-guoda/preston/issues/139/events","html_url":"https://github.com/bio-guoda/preston/issues/139","id":1029413013,"node_id":"I_kwDOCMmefs49W5iV","number":139,"title":"show example on how to process an image corpus offline or offsite","user":{"login":"jhpoelen","id":1084872,"node_id":"MDQ6VXNlcjEwODQ4NzI=","avatar_url":"https://avatars.githubusercontent.com/u/1084872?v=4","gravatar_id":"","url":"https://api.github.com/users/jhpoelen","html_url":"https://github.com/jhpoelen","followers_url":"https://api.github.com/users/jhpoelen/followers","following_url":"https://api.github.com/users/jhpoelen/following{/other_user}","gists_url":"https://api.github.com/users/jhpoelen/gists{/gist_id}","starred_url":"https://api.github.com/users/jhpoelen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jhpoelen/subscriptions","organizations_url":"https://api.github.com/users/jhpoelen/orgs","repos_url":"https://api.github.com/users/jhpoelen/repos","events_url":"https://api.github.com/users/jhpoelen/events{/privacy}","received_events_url":"https://api.github.com/users/jhpoelen/received_events","type":"User","user_view_type":"public","site_admin":false},"labels":[{"id":1045930075,"node_id":"MDU6TGFiZWwxMDQ1OTMwMDc1","url":"https://api.github.com/repos/bio-guoda/preston/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":28,"created_at":"2021-10-18T17:43:16Z","updated_at":"2023-06-02T15:44:44Z","closed_at":null,"author_association":"MEMBER","type":null,"active_lock_reason":null,"sub_issues_summary":{"total":0,"completed":0,"percent_completed":0},"issue_dependencies_summary":{"blocked_by":0,"total_blocked_by":0,"blocking":0,"total_blocking":0},"body":"related to @matdillen conversation at TDWG 2021 conference - \r\n\r\n\r\nsamples from live chat:\r\n\r\nJorrit Poelen\r\n```@Quentin``` - I've experimented with streaming approaching to processing large image datasets without have to store all if it in one location.\r\nPreliminary results suggests that with limited compute /store resources, you can still analyze a large well-defined corpus of images of known provenance. You might already be familiar with the small example at https://jhpoelen.nl/bees . . \r\n\r\nMathias Dillen\r\n```@Jorrit```: I noted before that some work on this was done in iDigBio, e.g. this abstract https://doi.org/10.3897/biss.2.25699 But was there any follow-up or other documentation? I've not been able to find any.\r\n\r\nJorrit Poelen\r\nThe Collins et al. work led up to the prototype I shared earlier. The earlier approach was described in Thessen, A.E. et al., 2018. 20 GB in 10 minutes: a case for linking major biodiversity databases using an open socio-technical infrastructure and a pragmatic, cross-institutional collaboration. PeerJ Computer Science .\r\n\r\nDocumentation of current approach (decentralized, verifiable, content-based data archives), can be found via https://jhpoelen.nl/bees and https://preston.guoda.bio .Happy to share more information if needed.\r\n\r\nErica Krimmel\r\n```@Mathias```, the GUODA resource referenced in that abstract is no longer active, but like Jorrit said has led to other solutions. Another, ad-hoc is to work with everything in the same cloud environment, like Google Collab + FigShare. Super fast access to image data and cloud computing. An example of that is this workshop the Smithsonian and iDigBio did at the last Botany conference: https://github.com/richiehodel/Botany2021_DLworkshop \r\n^ that approach does involve storing images all in one place, although not using your personal computing resources to do so\r\n\r\nJorrit Poelen\r\nNote that many of the earlier instances of the GUODA infrastructure have been adopted by GBIF and other infrastructure. E.g., Apache Spark, Parquet formatted datasets, Jupyter Notebook, HDFS etc. I found these tools are powerful, but very expensive to maintain. . . and ironically, don't scale that well because of it.\r\n10:03 AM\r\n(many of the earlier instances of the GUODA infrastructure) -> (earlier technologies used in earlier instanced of ... )\r\n\r\nJorrit Poelen\r\n10:30 AM\r\n```@Mathias / Erica``` - I'd love to have a live discussion on making it easier to access and cite image corpora. This is an active field of development, and hearing your perspective would be very useful. Also, perhaps we already have pragmatic solutions to make it easier to already analyze large image corpora without have to spend $$$ on hardware/staffing/network etc. \r\n\r\n```@Jorrit and Erica```: thanks for the feedback. I wonder if work continued with offering the possibility to apply ML algorithms to iDigBio images (or other image stores) without a requirement to download/stream the images elsewhere? Erica's example still required copying the images to google drive, for example.\r\n\r\nMathias Dillen\r\n```@Jorrit```: Yes, we should have that discussion. We have an increasing trove of images, sitting on local servers and therefore difficult to process in bulk without local access (and hardware!).\r\n\r\nErica Krimmel\r\n+1 Jorrit and Mathias! I would get a lot out of hearing others' thoughts on this topic.\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/bio-guoda/preston/issues/139/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/bio-guoda/preston/issues/139/timeline","performed_via_github_app":null,"state_reason":null}